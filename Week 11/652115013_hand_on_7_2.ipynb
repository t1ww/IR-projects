{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand on 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `22-27` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 22 : (but optimized?)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Precompile regex patterns (compiled only once)\n",
    "_var_addr_pattern = re.compile(r'.*0x[0-9a-f].*')\n",
    "_name_with_number_pattern = re.compile(r'.*[a-f]*:[0-9]*')\n",
    "_number_start_one_char_pattern = re.compile(r'[a-f][0-9].*')\n",
    "_number_start_three_char_pattern = re.compile(r'[a-f]{3}[0-9].*')\n",
    "_number_sub_pattern = re.compile(r'[\\\\/;:_-]')\n",
    "\n",
    "def preprocess(text, stopword_set, stemmer):\n",
    "    # Remove punctuation and unwanted characters, then lowercase the text\n",
    "    translation_table = str.maketrans('', '', '!\"#$%&\\'()*+,.<=>?@[]^`{|}~' + u'\\xa0')\n",
    "    cleaned_text = text.translate(translation_table).lower()\n",
    "    \n",
    "    # Replace all whitespace characters with a single space\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans(string.whitespace, ' ' * len(string.whitespace), ''))\n",
    "    \n",
    "    # Split the text only once\n",
    "    tokens = cleaned_text.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    # Process each token with all transformation rules in one pass\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            new_tokens.append('_variable_with_underscore')\n",
    "        elif '-' in token:\n",
    "            new_tokens.append('_variable_with_dash')\n",
    "        elif len(token) > 15 and token[0] != '#':\n",
    "            new_tokens.append('_long_variable_name')\n",
    "        elif token.startswith('http') and '/' in token:\n",
    "            new_tokens.append('_weburl')\n",
    "        elif _number_sub_pattern.sub('', token).isdigit():\n",
    "            new_tokens.append('_number')\n",
    "        elif _var_addr_pattern.match(token):\n",
    "            new_tokens.append('_variable_with_address')\n",
    "        elif _name_with_number_pattern.match(token):\n",
    "            new_tokens.append('_name_with_number')\n",
    "        elif _number_start_one_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_one_character')\n",
    "        elif _number_start_three_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_three_characters')\n",
    "        elif any(c.isdigit() for c in token) and token.startswith('v'):\n",
    "            new_tokens.append('_version')\n",
    "        elif ('\\\\' in token or '/' in token) and ':' not in token:\n",
    "            new_tokens.append('_localpath')\n",
    "        elif token.endswith('px'):\n",
    "            new_tokens.append('_image_size')\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    # Remove stopwords and tokens shorter than 3 characters, then perform stemming\n",
    "    final_tokens = [stemmer.stem(tok) for tok in new_tokens if tok not in stopword_set and len(tok) > 2]\n",
    "    return ' '.join(final_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this takes too long\n",
    "# # Page 23\n",
    "# import pandas as pd\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from multiprocessing import Pool\n",
    "# \n",
    "# # Function to initialize global variables in worker processes\n",
    "# def initialize_pool(stopword_set_arg, stemmer_arg):\n",
    "#     global stopword_set, stemmer\n",
    "#     stopword_set = stopword_set_arg\n",
    "#     stemmer = stemmer_arg\n",
    "# \n",
    "# # Load dataset\n",
    "# dataset = pd.read_json('../Week 10/resource/embold_train.json')\n",
    "# \n",
    "# # Label transformations\n",
    "# dataset.loc[dataset['label'] > 0, 'label'] = -1\n",
    "# dataset.loc[dataset['label'] == 0, 'label'] = 1\n",
    "# dataset.loc[dataset['label'] == -1, 'label'] = 0\n",
    "# \n",
    "# # Define stopwords and stemmer\n",
    "# stopwords_set = set(stopwords.words('English'))\n",
    "# ps = PorterStemmer()\n",
    "# \n",
    "# # Initialize the pool of workers with the optimized preprocess globals\n",
    "# pool = Pool(8, initializer=initialize_pool, initargs=(stopwords_set, ps))\n",
    "# \n",
    "# # Preprocess the dataset using multiprocessing\n",
    "# cleaned_title = pool.map(preprocess, dataset['title'])\n",
    "# cleaned_body = pool.map(preprocess, dataset['body'])\n",
    "# \n",
    "# \n",
    "# # Combine the cleaned texts into a DataFrame\n",
    "# data_texts = pd.DataFrame({'title': cleaned_title, 'body': cleaned_body})\n",
    "# \n",
    "# # Labels\n",
    "# y = dataset['label']\n",
    "# \n",
    "# # Close the pool\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle is now given, Skipping first 2 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping page 23\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "# Read pickle\n",
    "with open('../Week 10/resource/data_texts.pickle', 'rb') as f:\n",
    "    data_texts = pickle.load(f)\n",
    "with open('../Week 10/resource/embold_train_y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: p:0.7962 r:0.7935 f:0.7946\n"
     ]
    }
   ],
   "source": [
    "# Page 25 : Walkthroughs – cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Split the dataset into training and blindtest (testing) sets\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = train_test_split(data_texts, y, test_size=0.1)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with unigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Concatenate the title and body columns\n",
    "# Assuming 'data_texts' contains both 'title' and 'body' columns\n",
    "data_texts_combined = data_texts['title'] + ' ' + data_texts['body']\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the concatenated text (title + body)\n",
    "tfidf_vectorizer.fit(data_texts_combined)\n",
    "\n",
    "# Transform the training and blindtest data\n",
    "X_tfidf_fit = tfidf_vectorizer.transform(data_fit['title'] + ' ' + data_fit['body'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'] + ' ' + data_blindtest['body'])\n",
    "\n",
    "# Initialize the model\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "\n",
    "# Cross-validation for precision, recall, and f1 score\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "# Output the results\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 32720, number of negative: 40780\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 85138\n",
      "[LightGBM] [Info] Number of data points in the train set: 73500, number of used features: 1864\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445170 -> initscore=-0.220205\n",
      "[LightGBM] [Info] Start training from score -0.220205\n",
      "test: p:0.7465 r:0.7704 f:0.7494\n"
     ]
    }
   ],
   "source": [
    "# Page 26 : Modelling\n",
    "from sklearn import metrics\n",
    "\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = model_selection.train_test_split(data_texts, y, test_size=0.3)\n",
    "\n",
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3)\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train['title'])\n",
    "X_tfidf_fit_test = tfidf_vectorizer.transform(data_fit_test['title'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'])\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Page 27 : Save the models\n",
    "pickle.dump(tfidf_vectorizer, open('../Week 10/resource/github_bug_prediction_tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(gbm_model, open('../Week 10/resource/github_bug_prediction_basic_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `28-29` of Handout #7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  # Using PorterStemmer\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load resources\n",
    "app.tfidf_vectorizer = pickle.load(open('../Week 10/resource/github_bug_prediction_tfidf_vectorizer.pkl', 'rb'))\n",
    "app.basic_model = pickle.load(open('../Week 10/resource/github_bug_prediction_basic_model.pkl', 'rb'))\n",
    "app.stopword_set = set(stopwords.words('english'))\n",
    "app.stemmer = PorterStemmer()  # Correctly initialize PorterStemmer\n",
    "\n",
    "@app.route('/predict_basic', methods=['GET'])\n",
    "def predict_basic_get():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get query parameters\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    title = argList.get('title', [None])[0]  # Safely get title\n",
    "    body = argList.get('body', [None])[0]  # Safely get body\n",
    "\n",
    "    if not title or not body:  # Validate input\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Apply preprocessing to the title\n",
    "    processed_title = preprocess(title)  # Using preprocess function on the title\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n",
    "\n",
    "@app.route('/predict_basic', methods=['POST'])\n",
    "def predict_basic_post():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get data from JSON body\n",
    "    data = request.get_json()  # Parse JSON body\n",
    "    title = data.get('title')  \n",
    "    body = data.get('body')    \n",
    "\n",
    "    if not title or not body:  # Validate if title or body are missing\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Initialize stopword_set and stemmer\n",
    "    stopword_set = set(stopwords.words('english'))  # Assuming you are using NLTK stopwords\n",
    "    stemmer = PorterStemmer()  # Using PorterStemmer\n",
    "    \n",
    "    # Process title using preprocess with stopword_set and stemmer\n",
    "    processed_title = preprocess(title, stopword_set, stemmer)\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Feb/2025 17:38:15] \"POST /predict_basic HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hand on 7.2\n",
    "Page `39` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (135000, 500)\n",
      "y: (105000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'x: {X_lsa_fit.shape}')\n",
    "print(f'y: {y_fit.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "lsa = TruncatedSVD(n_components=500, n_iter=100, random_state=0)\n",
    "lsa.fit(X_tfidf_fit)\n",
    "X_lsa_fit = lsa.transform(X_tfidf_fit)\n",
    "\n",
    "gbm_model_with_lsa = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA fit: p:0.4972 r:0.4993 f:0.4077\n"
     ]
    }
   ],
   "source": [
    "X_lsa_fit = X_lsa_fit[:len(y_fit)]  # Trim X_lsa_fit to match the length of y_fit\n",
    "X_tfidf_fit = X_tfidf_fit[:len(y_fit)]  # Trim X_tfidf_fit to match the length of y_fit\n",
    "\n",
    "# Cross-validation using only LSA features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('LSA fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With LSA and original features: p:0.5030 r:0.5009 f:0.4119\n"
     ]
    }
   ],
   "source": [
    "# Stack LSA features with original features\n",
    "X_fit_with_lsa = hstack([X_tfidf_fit, X_lsa_fit]).tocsr()\n",
    "\n",
    "# Cross-validation using both LSA and original features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('With LSA and original features: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `43` of handout 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit: p:0.6833 r:0.6724 f:0.6734\n",
      "fit: p:0.6812 r:0.6704 f:0.6714\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load 'title' and 'body' columns\n",
    "cleaned_title = data_texts['title']\n",
    "cleaned_body = data_texts['body']\n",
    "\n",
    "# Initialize and fit CountVectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "\n",
    "# Perform transformation\n",
    "X_tf_fit = count_vectorizer.transform(data_fit['title'] + data_fit['body'])\n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest['title'] + data_blindtest['body'])\n",
    "\n",
    "# Perform Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0)\n",
    "lda.fit(X_tf_fit)\n",
    "X_lda_fit = lda.transform(X_tf_fit)\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "gbm_model_with_lda = lgb.LGBMClassifier()\n",
    "\n",
    "# Perform cross-validation with LDA-transformed data\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "# Combine original TF-IDF features with LDA-transformed features\n",
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr()\n",
    "\n",
    "# Perform cross-validation with combined features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `45` of Handout 7\n",
    "\n",
    "In class activity\n",
    "- Make a TF-IDF + LSA + LDA version\n",
    "- Please carefully design the dataflow\n",
    "- Raw data -> TF-IDF vectorizer -> basic … (1)\n",
    "- Raw data -> TF-IDF vectorizer -> LSA … (2)\n",
    "- Raw data -> TF vectorizer -> LDA … (3)\n",
    "- GBM( 1 + 2 + 3 ) -> predicted probability\n",
    "- Make this TF-IDF + LSA + LDA a flask application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we build the Basic & LSA Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Assume data_texts is a DataFrame with 'title' and 'body' columns and y are the labels.\n",
    "# Combine title and body\n",
    "data_texts_combined = data_texts['title'] + ' ' + data_texts['body']\n",
    "\n",
    "# Pipeline (1): TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data_texts_combined)\n",
    "\n",
    "# Pipeline (2): LSA on TF-IDF features\n",
    "# We use TruncatedSVD to reduce dimensionality (i.e. perform LSA)\n",
    "lsa = TruncatedSVD(n_components=500, n_iter=100, random_state=0)\n",
    "# Fit LSA on the TF-IDF matrix and transform it:\n",
    "X_lsa = lsa.fit_transform(X_tfidf)  # This gives a dense matrix\n",
    "\n",
    "# Convert LSA output to a sparse format so it can be hstacked with X_tfidf:\n",
    "X_lsa_sparse = csr_matrix(X_lsa)\n",
    "\n",
    "# At this point, pipelines (1) and (2) are built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we build the TF + LDA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Pipeline (3): Use a CountVectorizer (TF) for raw counts\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "X_tf = count_vectorizer.fit_transform(data_texts_combined)\n",
    "\n",
    "# Apply LDA on the count matrix to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0)\n",
    "# Fit LDA and transform the count matrix:\n",
    "X_lda = lda.fit_transform(X_tf)  # This is dense (each row = topic distribution)\n",
    "\n",
    "# Convert LDA output to sparse format for stacking:\n",
    "X_lda_sparse = csr_matrix(X_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Features and Train the Combined GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features from all three pipelines:\n",
    "# (1) TF-IDF (basic), (2) LSA features, (3) LDA topic distribution\n",
    "X_combined = hstack([X_tfidf, X_lsa_sparse, X_lda_sparse]).tocsr()\n",
    "\n",
    "# Optionally, ensure X_combined and y have matching dimensions (e.g., slicing if needed)\n",
    "X_combined = X_combined[:len(y)]\n",
    "\n",
    "# Train/test split for evaluation (or cross-validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Gradient Boosting Model (or LightGBM, as preferred) on the combined features:\n",
    "gbm_combined = GradientBoostingClassifier()\n",
    "gbm_combined.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate using cross-validation (example: F1 macro score)\n",
    "cv_score = cross_val_score(gbm_combined, X_combined, y, cv=5, scoring='f1_macro').mean()\n",
    "print(\"Combined model cross-validation F1 score:\", cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Save the Trained Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(lsa, open('lsa.pkl', 'wb'))\n",
    "pickle.dump(count_vectorizer, open('count_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(lda, open('lda.pkl', 'wb'))\n",
    "pickle.dump(gbm_combined, open('gbm_combined.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make Flask application for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the pre-trained components\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lsa = pickle.load(open('lsa.pkl', 'rb'))\n",
    "count_vectorizer = pickle.load(open('count_vectorizer.pkl', 'rb'))\n",
    "lda = pickle.load(open('lda.pkl', 'rb'))\n",
    "gbm_combined = pickle.load(open('gbm_combined.pkl', 'rb'))\n",
    "\n",
    "@app.route('/predict/combined', methods=['POST'])\n",
    "def predict_combined():\n",
    "    # Expect input as JSON: {\"text\": \"raw text here\"}\n",
    "    data = request.get_json()\n",
    "    raw_text = data['text']\n",
    "    \n",
    "    # Prepare the combined input:\n",
    "    # 1. Use TF-IDF vectorizer for pipeline (1)\n",
    "    X_tfidf_input = tfidf_vectorizer.transform([raw_text])\n",
    "    \n",
    "    # 2. Get LSA features from TF-IDF\n",
    "    X_lsa_input = lsa.transform(X_tfidf_input)\n",
    "    X_lsa_input_sparse = csr_matrix(X_lsa_input)\n",
    "    \n",
    "    # 3. For LDA, use CountVectorizer (TF)\n",
    "    X_tf_input = count_vectorizer.transform([raw_text])\n",
    "    X_lda_input = lda.transform(X_tf_input)\n",
    "    X_lda_input_sparse = csr_matrix(X_lda_input)\n",
    "    \n",
    "    # Combine the three representations\n",
    "    X_combined_input = hstack([X_tfidf_input, X_lsa_input_sparse, X_lda_input_sparse]).tocsr()\n",
    "    \n",
    "    # Get predicted probability from the combined GBM model\n",
    "    predicted_probability = gbm_combined.predict_proba(X_combined_input)[0]\n",
    "    \n",
    "    return jsonify({'predicted_probability': predicted_probability.tolist()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run application here\n",
    "app.run(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
