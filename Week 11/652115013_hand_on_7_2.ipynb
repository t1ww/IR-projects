{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand on 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `22-27` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 22 : (but optimized?)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Precompile regex patterns (compiled only once)\n",
    "_var_addr_pattern = re.compile(r'.*0x[0-9a-f].*')\n",
    "_name_with_number_pattern = re.compile(r'.*[a-f]*:[0-9]*')\n",
    "_number_start_one_char_pattern = re.compile(r'[a-f][0-9].*')\n",
    "_number_start_three_char_pattern = re.compile(r'[a-f]{3}[0-9].*')\n",
    "_number_sub_pattern = re.compile(r'[\\\\/;:_-]')\n",
    "\n",
    "def preprocess(text, stopword_set, stemmer):\n",
    "    # Remove punctuation and unwanted characters, then lowercase the text\n",
    "    translation_table = str.maketrans('', '', '!\"#$%&\\'()*+,.<=>?@[]^`{|}~' + u'\\xa0')\n",
    "    cleaned_text = text.translate(translation_table).lower()\n",
    "    \n",
    "    # Replace all whitespace characters with a single space\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans(string.whitespace, ' ' * len(string.whitespace), ''))\n",
    "    \n",
    "    # Split the text only once\n",
    "    tokens = cleaned_text.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    # Process each token with all transformation rules in one pass\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            new_tokens.append('_variable_with_underscore')\n",
    "        elif '-' in token:\n",
    "            new_tokens.append('_variable_with_dash')\n",
    "        elif len(token) > 15 and token[0] != '#':\n",
    "            new_tokens.append('_long_variable_name')\n",
    "        elif token.startswith('http') and '/' in token:\n",
    "            new_tokens.append('_weburl')\n",
    "        elif _number_sub_pattern.sub('', token).isdigit():\n",
    "            new_tokens.append('_number')\n",
    "        elif _var_addr_pattern.match(token):\n",
    "            new_tokens.append('_variable_with_address')\n",
    "        elif _name_with_number_pattern.match(token):\n",
    "            new_tokens.append('_name_with_number')\n",
    "        elif _number_start_one_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_one_character')\n",
    "        elif _number_start_three_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_three_characters')\n",
    "        elif any(c.isdigit() for c in token) and token.startswith('v'):\n",
    "            new_tokens.append('_version')\n",
    "        elif ('\\\\' in token or '/' in token) and ':' not in token:\n",
    "            new_tokens.append('_localpath')\n",
    "        elif token.endswith('px'):\n",
    "            new_tokens.append('_image_size')\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    # Remove stopwords and tokens shorter than 3 characters, then perform stemming\n",
    "    final_tokens = [stemmer.stem(tok) for tok in new_tokens if tok not in stopword_set and len(tok) > 2]\n",
    "    return ' '.join(final_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 23\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to initialize global variables in worker processes\n",
    "def initialize_pool(stopword_set_arg, stemmer_arg):\n",
    "    global stopword_set, stemmer\n",
    "    stopword_set = stopword_set_arg\n",
    "    stemmer = stemmer_arg\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_json('../Week 10/resource/embold_train.json')\n",
    "\n",
    "# Label transformations\n",
    "dataset.loc[dataset['label'] > 0, 'label'] = -1\n",
    "dataset.loc[dataset['label'] == 0, 'label'] = 1\n",
    "dataset.loc[dataset['label'] == -1, 'label'] = 0\n",
    "\n",
    "# Define stopwords and stemmer\n",
    "stopwords_set = set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Initialize the pool of workers with the optimized preprocess globals\n",
    "pool = Pool(8, initializer=initialize_pool, initargs=(stopwords_set, ps))\n",
    "\n",
    "# Preprocess the dataset using multiprocessing\n",
    "cleaned_title = pool.map(preprocess, dataset['title'])\n",
    "cleaned_body = pool.map(preprocess, dataset['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the cleaned texts into a DataFrame\n",
    "data_texts = pd.DataFrame({'title': cleaned_title, 'body': cleaned_body})\n",
    "\n",
    "# Labels\n",
    "y = dataset['label']\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle is now given, Skipping first 2 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping page 23\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "# Read pickle\n",
    "with open('../Week 10/resource/data_texts.pickle', 'rb') as f:\n",
    "    data_texts = pickle.load(f)\n",
    "with open('../Week 10/resource/embold_train_y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: p:0.7964 r:0.7937 f:0.7948\n"
     ]
    }
   ],
   "source": [
    "# Page 25 : Walkthroughs – cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Split the dataset into training and blindtest (testing) sets\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = train_test_split(data_texts, y, test_size=0.1)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with unigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Concatenate the title and body columns\n",
    "# Assuming 'data_texts' contains both 'title' and 'body' columns\n",
    "data_texts_combined = data_texts['title'] + ' ' + data_texts['body']\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the concatenated text (title + body)\n",
    "tfidf_vectorizer.fit(data_texts_combined)\n",
    "\n",
    "# Transform the training and blindtest data\n",
    "X_tfidf_fit = tfidf_vectorizer.transform(data_fit['title'] + ' ' + data_fit['body'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'] + ' ' + data_blindtest['body'])\n",
    "\n",
    "# Initialize the model\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "\n",
    "# Cross-validation for precision, recall, and f1 score\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "# Output the results\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 32752, number of negative: 40748\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 85039\n",
      "[LightGBM] [Info] Number of data points in the train set: 73500, number of used features: 1856\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.445605 -> initscore=-0.218443\n",
      "[LightGBM] [Info] Start training from score -0.218443\n",
      "test: p:0.7439 r:0.7670 f:0.7470\n"
     ]
    }
   ],
   "source": [
    "# Page 26 : Modelling\n",
    "from sklearn import metrics\n",
    "\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = model_selection.train_test_split(data_texts, y, test_size=0.3)\n",
    "\n",
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3)\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train['title'])\n",
    "X_tfidf_fit_test = tfidf_vectorizer.transform(data_fit_test['title'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'])\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Page 27 : Save the models\n",
    "pickle.dump(tfidf_vectorizer, open('../Week 10/resource/github_bug_prediction_tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(gbm_model, open('../Week 10/resource/github_bug_prediction_basic_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `28-29` of Handout #7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  # Using PorterStemmer\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load resources\n",
    "app.tfidf_vectorizer = pickle.load(open('../Week 10/resource/github_bug_prediction_tfidf_vectorizer.pkl', 'rb'))\n",
    "app.basic_model = pickle.load(open('../Week 10/resource/github_bug_prediction_basic_model.pkl', 'rb'))\n",
    "app.stopword_set = set(stopwords.words('english'))\n",
    "app.stemmer = PorterStemmer()  # Correctly initialize PorterStemmer\n",
    "\n",
    "@app.route('/predict_basic', methods=['GET'])\n",
    "def predict_basic_get():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get query parameters\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    title = argList.get('title', [None])[0]  # Safely get title\n",
    "    body = argList.get('body', [None])[0]  # Safely get body\n",
    "\n",
    "    if not title or not body:  # Validate input\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Apply preprocessing to the title\n",
    "    processed_title = preprocess(title)  # Using preprocess function on the title\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n",
    "\n",
    "@app.route('/predict_basic', methods=['POST'])\n",
    "def predict_basic_post():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get data from JSON body\n",
    "    data = request.get_json()  # Parse JSON body\n",
    "    title = data.get('title')  \n",
    "    body = data.get('body')    \n",
    "\n",
    "    if not title or not body:  # Validate if title or body are missing\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Initialize stopword_set and stemmer\n",
    "    stopword_set = set(stopwords.words('english'))  # Assuming you are using NLTK stopwords\n",
    "    stemmer = PorterStemmer()  # Using PorterStemmer\n",
    "    \n",
    "    # Process title using preprocess with stopword_set and stemmer\n",
    "    processed_title = preprocess(title, stopword_set, stemmer)\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Feb/2025 17:38:15] \"POST /predict_basic HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hand on 7.2\n",
    "Page `39` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (135000, 500)\n",
      "y: (105000,)\n"
     ]
    }
   ],
   "source": [
    "print(f'x: {X_lsa_fit.shape}')\n",
    "print(f'y: {y_fit.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "lsa = TruncatedSVD(n_components=500, n_iter=100, random_state=0)\n",
    "lsa.fit(X_tfidf_fit)\n",
    "X_lsa_fit = lsa.transform(X_tfidf_fit)\n",
    "\n",
    "gbm_model_with_lsa = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA fit: p:0.4996 r:0.4999 f:0.4098\n"
     ]
    }
   ],
   "source": [
    "X_lsa_fit = X_lsa_fit[:len(y_fit)]  # Trim X_lsa_fit to match the length of y_fit\n",
    "X_tfidf_fit = X_tfidf_fit[:len(y_fit)]  # Trim X_tfidf_fit to match the length of y_fit\n",
    "\n",
    "# Cross-validation using only LSA features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('LSA fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With LSA and original features: p:0.5021 r:0.5006 f:0.4121\n"
     ]
    }
   ],
   "source": [
    "# Stack LSA features with original features\n",
    "X_fit_with_lsa = hstack([X_tfidf_fit, X_lsa_fit]).tocsr()\n",
    "\n",
    "# Cross-validation using both LSA and original features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('With LSA and original features: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `43` of handout 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize and fit CountVectorizer\u001b[39;00m\n\u001b[0;32m      8\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m count_vectorizer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mcleaned_title\u001b[49m \u001b[38;5;241m+\u001b[39m cleaned_body)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Transform data using the fitted vectorizer\u001b[39;00m\n\u001b[0;32m     12\u001b[0m X_tf_fit \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mtransform(data_fit[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_title' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the pickled data\n",
    "with open('../Week 10/resource/data_texts.pickle', 'rb') as f:\n",
    "    data_texts = pickle.load(f)\n",
    "with open('../Week 10/resource/embold_train_y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "# Assuming 'data_texts' contains 'title' and 'body' columns\n",
    "cleaned_title = data_texts['title']\n",
    "cleaned_body = data_texts['body']\n",
    "\n",
    "# Initialize and fit CountVectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "\n",
    "# Perform transformation\n",
    "X_tf_fit = count_vectorizer.transform(data_fit['title'] + data_fit['body'])\n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest['title'] + data_blindtest['body'])\n",
    "\n",
    "# Perform Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=500, random_state=0)\n",
    "lda.fit(X_tf_fit)\n",
    "X_lda_fit = lda.transform(X_tf_fit)\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "gbm_model_with_lda = lgb.LGBMClassifier()\n",
    "\n",
    "# Perform cross-validation with LDA-transformed data\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "# Combine original TF-IDF features with LDA-transformed features\n",
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr()\n",
    "\n",
    "# Perform cross-validation with combined features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `45` of Handout 7\n",
    "\n",
    "In class activity\n",
    "- Make a TF-IDF + LSA + LDA version\n",
    "- Please carefully design the dataflow\n",
    "- Raw data -> TF-IDF vectorizer -> basic … (1)\n",
    "- Raw data -> TF-IDF vectorizer -> LSA … (2)\n",
    "- Raw data -> TF vectorizer -> LDA … (3)\n",
    "- GBM( 1 + 2 + 3 ) -> predicted probability\n",
    "- Make this TF-IDF + LSA + LDA a flask application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
