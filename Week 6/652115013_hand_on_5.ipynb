{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand on 5 (1 of 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import multiprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Empty\n",
    "from pathlib import Path\n",
    "from tokenize import Comment\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from multiprocessing import Queue\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`Page 29 - 33 of Handout #5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple multithreaded web crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 29\n",
    "class MultiThreadCrawler:\n",
    "    def __init__(self, base_url, depth):\n",
    "        self.base_url = base_url\n",
    "        extracted_url = urlparse(base_url)\n",
    "        parent = extracted_url.path[:extracted_url.path.rfind(\"/\") + 1]\n",
    "        self.root_url = '{}://{}{}'.format(extracted_url.scheme, extracted_url.netloc, parent)\n",
    "        self.pool = ThreadPoolExecutor(max_workers=multiprocessing.cpu_count() - 1)\n",
    "        self.to_crawl = Queue()\n",
    "        self.to_crawl.put({self.base_url: depth})\n",
    "        self.stored_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "\n",
    "        if not Path(self.stored_folder).exists():\n",
    "            Path.mkdir(self.stored_folder)\n",
    "\n",
    "        if Path(self.stored_folder / 'url_list.pickle').exists():\n",
    "            with open(self.stored_folder / 'url_list.pickle', 'rb') as f:\n",
    "                self.crawled_pages = pickle.load(f)\n",
    "            print(self.crawled_pages)\n",
    "        else:\n",
    "            self.crawled_pages = set([])\n",
    "    \n",
    "    # Page 30\n",
    "    def extract_page(self, obj):\n",
    "        if obj.result():\n",
    "            result, url, depth = obj.result()\n",
    "        if result and result.status_code == 200:\n",
    "            url_lists = self.parse_links(result.text, depth)\n",
    "            self.parse_contents(url, result.text, url_lists)\n",
    "    def get_page(self, url, depth):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=(3, 30))\n",
    "            return res, url, depth\n",
    "        except requests.RequestException:\n",
    "            return\n",
    "        \n",
    "    # Page 31\n",
    "    def parse_links(self, html, depth):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = soup.find_all('a', href=True)\n",
    "        url_lists = []\n",
    "        for link in links:\n",
    "            url = link['href']\n",
    "            url = urljoin(self.root_url, url)\n",
    "            if depth >= 0 and '..' not in url and url not in self.crawled_pages:\n",
    "                print(\"Adding {}\".format(url))\n",
    "                self.to_crawl.put({url: depth})\n",
    "            url_lists.append(url)\n",
    "        return url_lists\n",
    "    \n",
    "    # Page 32\n",
    "    def parse_contents(self, url, html, url_lists):\n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            texts = soup.findAll(string=True)\n",
    "            visible_texts = filter(tag_visible, texts)\n",
    "\n",
    "            title = soup.find('title').string.strip()\n",
    "            text = u\" \".join(t.strip() for t in visible_texts).strip()\n",
    "\n",
    "            with open(self.stored_folder / (str(hash(url)) + '.txt'), 'w', encoding='utf-8') as f:\n",
    "                json.dump({'url': url, 'title': title, 'text': text, 'url_lists': url_lists}, f, ensure_ascii=False)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # Page 33\n",
    "    def run_scraper(self):\n",
    "        while True:\n",
    "            try:\n",
    "                target = self.to_crawl.get(timeout=10)\n",
    "                url, depth = [(k, target[k]) for k in target][0]\n",
    "                if url not in self.crawled_pages:\n",
    "                    self.crawled_pages.add(url)\n",
    "                    job = self.pool.submit(self.get_page, url, depth - 1)\n",
    "                    job.add_done_callback(self.extract_page)\n",
    "            except Empty:\n",
    "                with open(self.stored_folder / 'url_list.pickle', 'wb') as f:\n",
    "                    pickle.dump(self.crawled_pages, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open(self.stored_folder / 'url_list.pickle', 'rb') as f:\n",
    "                    print(pickle.load(f))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://camt.cmu.ac.th/index.php/en/'}\n",
      "{'https://camt.cmu.ac.th/index.php/en/'}\n"
     ]
    }
   ],
   "source": [
    "# Page 33\n",
    "if __name__ == '__main__':\n",
    "    s = MultiThreadCrawler(\"https://www.cmu.ac.th/en/home\", 2)\n",
    "    s.run_scraper()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
