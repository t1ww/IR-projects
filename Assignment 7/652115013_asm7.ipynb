{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand on 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `22-27` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 22 : (but optimized?)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Precompile regex patterns (compiled only once)\n",
    "_var_addr_pattern = re.compile(r'.*0x[0-9a-f].*')\n",
    "_name_with_number_pattern = re.compile(r'.*[a-f]*:[0-9]*')\n",
    "_number_start_one_char_pattern = re.compile(r'[a-f][0-9].*')\n",
    "_number_start_three_char_pattern = re.compile(r'[a-f]{3}[0-9].*')\n",
    "_number_sub_pattern = re.compile(r'[\\\\/;:_-]')\n",
    "\n",
    "def preprocess(text, stopword_set, stemmer):\n",
    "    # Remove punctuation and unwanted characters, then lowercase the text\n",
    "    translation_table = str.maketrans('', '', '!\"#$%&\\'()*+,.<=>?@[]^`{|}~' + u'\\xa0')\n",
    "    cleaned_text = text.translate(translation_table).lower()\n",
    "    \n",
    "    # Replace all whitespace characters with a single space\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans(string.whitespace, ' ' * len(string.whitespace), ''))\n",
    "    \n",
    "    # Split the text only once\n",
    "    tokens = cleaned_text.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    # Process each token with all transformation rules in one pass\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            new_tokens.append('_variable_with_underscore')\n",
    "        elif '-' in token:\n",
    "            new_tokens.append('_variable_with_dash')\n",
    "        elif len(token) > 15 and token[0] != '#':\n",
    "            new_tokens.append('_long_variable_name')\n",
    "        elif token.startswith('http') and '/' in token:\n",
    "            new_tokens.append('_weburl')\n",
    "        elif _number_sub_pattern.sub('', token).isdigit():\n",
    "            new_tokens.append('_number')\n",
    "        elif _var_addr_pattern.match(token):\n",
    "            new_tokens.append('_variable_with_address')\n",
    "        elif _name_with_number_pattern.match(token):\n",
    "            new_tokens.append('_name_with_number')\n",
    "        elif _number_start_one_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_one_character')\n",
    "        elif _number_start_three_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_three_characters')\n",
    "        elif any(c.isdigit() for c in token) and token.startswith('v'):\n",
    "            new_tokens.append('_version')\n",
    "        elif ('\\\\' in token or '/' in token) and ':' not in token:\n",
    "            new_tokens.append('_localpath')\n",
    "        elif token.endswith('px'):\n",
    "            new_tokens.append('_image_size')\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    # Remove stopwords and tokens shorter than 3 characters, then perform stemming\n",
    "    final_tokens = [stemmer.stem(tok) for tok in new_tokens if tok not in stopword_set and len(tok) > 2]\n",
    "    return ' '.join(final_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to initialize global variables in worker processes\n",
    "def initialize_pool(stopword_set_arg, stemmer_arg):\n",
    "    global stopword_set, stemmer\n",
    "    stopword_set = stopword_set_arg\n",
    "    stemmer = stemmer_arg\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_json('../Week 10/resource/embold_train.json')\n",
    "\n",
    "# Label transformations\n",
    "dataset.loc[dataset['label'] > 0, 'label'] = -1\n",
    "dataset.loc[dataset['label'] == 0, 'label'] = 1\n",
    "dataset.loc[dataset['label'] == -1, 'label'] = 0\n",
    "\n",
    "# Define stopwords and stemmer\n",
    "stopwords_set = set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Initialize the pool of workers with the optimized preprocess globals\n",
    "pool = Pool(8, initializer=initialize_pool, initargs=(stopwords_set, ps))\n",
    "\n",
    "# Preprocess the dataset using multiprocessing\n",
    "cleaned_title = pool.map(preprocess, dataset['title'])\n",
    "cleaned_body = pool.map(preprocess, dataset['body'])\n",
    "\n",
    "\n",
    "# Combine the cleaned texts into a DataFrame\n",
    "data_texts = pd.DataFrame({'title': cleaned_title, 'body': cleaned_body})\n",
    "\n",
    "# Labels\n",
    "y = dataset['label']\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 25 : Walkthroughs – cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Split the dataset into training and blindtest (testing) sets\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = train_test_split(data_texts, y, test_size=0.1)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with unigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Concatenate the title and body columns\n",
    "# Assuming 'data_texts' contains both 'title' and 'body' columns\n",
    "data_texts_combined = data_texts['title'] + ' ' + data_texts['body']\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the concatenated text (title + body)\n",
    "tfidf_vectorizer.fit(data_texts_combined)\n",
    "\n",
    "# Transform the training and blindtest data\n",
    "X_tfidf_fit = tfidf_vectorizer.transform(data_fit['title'] + ' ' + data_fit['body'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'] + ' ' + data_blindtest['body'])\n",
    "\n",
    "# Initialize the model\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "\n",
    "# Cross-validation for precision, recall, and f1 score\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "# Output the results\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 26 : Modelling\n",
    "from sklearn import metrics\n",
    "\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = model_selection.train_test_split(data_texts, y, test_size=0.3)\n",
    "\n",
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3)\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train['title'])\n",
    "X_tfidf_fit_test = tfidf_vectorizer.transform(data_fit_test['title'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'])\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Page 27 : Save the models\n",
    "pickle.dump(tfidf_vectorizer, open('../Week 10/resource/github_bug_prediction_tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(gbm_model, open('../Week 10/resource/github_bug_prediction_basic_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `28-29` of Handout #7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  # Using PorterStemmer\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load resources\n",
    "app.tfidf_vectorizer = pickle.load(open('../Week 10/resource/github_bug_prediction_tfidf_vectorizer.pkl', 'rb'))\n",
    "app.basic_model = pickle.load(open('../Week 10/resource/github_bug_prediction_basic_model.pkl', 'rb'))\n",
    "app.stopword_set = set(stopwords.words('english'))\n",
    "app.stemmer = PorterStemmer()  # Correctly initialize PorterStemmer\n",
    "\n",
    "@app.route('/predict_basic', methods=['GET'])\n",
    "def predict_basic_get():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get query parameters\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    title = argList.get('title', [None])[0]  # Safely get title\n",
    "    body = argList.get('body', [None])[0]  # Safely get body\n",
    "\n",
    "    if not title or not body:  # Validate input\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Apply preprocessing to the title\n",
    "    processed_title = preprocess(title)  # Using preprocess function on the title\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n",
    "\n",
    "@app.route('/predict_basic', methods=['POST'])\n",
    "def predict_basic_post():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get data from JSON body\n",
    "    data = request.get_json()  # Parse JSON body\n",
    "    title = data.get('title')  \n",
    "    body = data.get('body')    \n",
    "\n",
    "    if not title or not body:  # Validate if title or body are missing\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Initialize stopword_set and stemmer\n",
    "    stopword_set = set(stopwords.words('english'))  # Assuming you are using NLTK stopwords\n",
    "    stemmer = PorterStemmer()  # Using PorterStemmer\n",
    "    \n",
    "    # Process title using preprocess with stopword_set and stemmer\n",
    "    processed_title = preprocess(title, stopword_set, stemmer)\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "lsa = TruncatedSVD(n_components=50, n_iter=100, random_state=0)\n",
    "lsa.fit(X_tfidf_fit)\n",
    "X_lsa_fit = lsa.transform(X_tfidf_fit)\n",
    "\n",
    "gbm_model_with_lsa = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lsa_fit = X_lsa_fit[:len(y_fit)]  # Trim X_lsa_fit to match the length of y_fit\n",
    "X_tfidf_fit = X_tfidf_fit[:len(y_fit)]  # Trim X_tfidf_fit to match the length of y_fit\n",
    "\n",
    "# Cross-validation using only LSA features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_lsa_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('LSA fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack LSA features with original features\n",
    "X_fit_with_lsa = hstack([X_tfidf_fit, X_lsa_fit]).tocsr()\n",
    "\n",
    "# Cross-validation using both LSA and original features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lsa, X_fit_with_lsa, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('With LSA and original features: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `43` of handout 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import model_selection\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load 'title' and 'body' columns\n",
    "cleaned_title = data_texts['title']\n",
    "cleaned_body = data_texts['body']\n",
    "\n",
    "# Initialize and fit CountVectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(cleaned_title + cleaned_body)\n",
    "\n",
    "# Perform transformation\n",
    "X_tf_fit = count_vectorizer.transform(data_fit['title'] + data_fit['body'])\n",
    "X_tf_blindtest = count_vectorizer.transform(data_blindtest['title'] + data_blindtest['body'])\n",
    "\n",
    "# Perform Latent Dirichlet Allocation (LDA)\n",
    "lda = LatentDirichletAllocation(n_components=50, random_state=0)\n",
    "lda.fit(X_tf_fit)\n",
    "X_lda_fit = lda.transform(X_tf_fit)\n",
    "\n",
    "# Initialize LightGBM classifier\n",
    "gbm_model_with_lda = lgb.LGBMClassifier()\n",
    "\n",
    "# Perform cross-validation with LDA-transformed data\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_lda_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "# Combine original TF-IDF features with LDA-transformed features\n",
    "X_fit_with_lda = hstack([X_tfidf_fit, X_lda_fit]).tocsr()\n",
    "\n",
    "# Perform cross-validation with combined features\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='precision_macro'\n",
    ").mean()\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='recall_macro'\n",
    ").mean()\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model_with_lda, X_fit_with_lda, y_fit, cv=5, n_jobs=-2, scoring='f1_macro'\n",
    ").mean()\n",
    "\n",
    "print('fit: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `45` of Handout 7\n",
    "\n",
    "In class activity\n",
    "- Make a TF-IDF + LSA + LDA version\n",
    "- Please carefully design the dataflow\n",
    "- Raw data -> TF-IDF vectorizer -> basic … (1)\n",
    "- Raw data -> TF-IDF vectorizer -> LSA … (2)\n",
    "- Raw data -> TF vectorizer -> LDA … (3)\n",
    "- GBM( 1 + 2 + 3 ) -> predicted probability\n",
    "- Make this TF-IDF + LSA + LDA a flask application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we build the Basic & LSA Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Assume data_texts is a DataFrame with 'title' and 'body' columns and y are the labels.\n",
    "# Combine title and body\n",
    "data_texts_combined = data_texts['title'] + ' ' + data_texts['body']\n",
    "\n",
    "# Pipeline (1): TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data_texts_combined)\n",
    "\n",
    "# Pipeline (2): LSA on TF-IDF features\n",
    "# We use TruncatedSVD to reduce dimensionality (i.e. perform LSA)\n",
    "lsa = TruncatedSVD(n_components=50, n_iter=100, random_state=0)\n",
    "# Fit LSA on the TF-IDF matrix and transform it:\n",
    "X_lsa = lsa.fit_transform(X_tfidf)  # This gives a dense matrix\n",
    "\n",
    "# Convert LSA output to a sparse format so it can be hstacked with X_tfidf:\n",
    "X_lsa_sparse = csr_matrix(X_lsa)\n",
    "\n",
    "# At this point, pipelines (1) and (2) are built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we build the TF + LDA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Pipeline (3): Use a CountVectorizer (TF) for raw counts\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "X_tf = count_vectorizer.fit_transform(data_texts_combined)\n",
    "\n",
    "# Apply LDA on the count matrix to extract topics\n",
    "lda = LatentDirichletAllocation(n_components=50, random_state=0)\n",
    "# Fit LDA and transform the count matrix:\n",
    "X_lda = lda.fit_transform(X_tf)  # This is dense (each row = topic distribution)\n",
    "\n",
    "# Convert LDA output to sparse format for stacking:\n",
    "X_lda_sparse = csr_matrix(X_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Features and Train the Combined GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine features from all three pipelines: (1) TF-IDF (basic), (2) LSA features, (3) LDA topic distribution\n",
    "X_combined = hstack([X_tfidf, X_lsa_sparse, X_lda_sparse]).tocsr()\n",
    "\n",
    "# Optionally, ensure X_combined and y have matching dimensions (e.g., slicing if needed)\n",
    "X_combined = X_combined[:len(y)]\n",
    "\n",
    "# Train/test split for evaluation (or cross-validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a LightGBM model on the combined features:\n",
    "gbm_combined = LGBMClassifier(n_jobs=-1, random_state=42)\n",
    "gbm_combined.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate using cross-validation (example: F1 macro score)\n",
    "cv_score = cross_val_score(gbm_combined, X_combined, y, cv=5, scoring='f1_macro').mean()\n",
    "print(\"Combined model cross-validation F1 score:\", cv_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Save the Trained Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(lsa, open('lsa.pkl', 'wb'))\n",
    "pickle.dump(count_vectorizer, open('count_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(lda, open('lda.pkl', 'wb'))\n",
    "pickle.dump(gbm_combined, open('gbm_combined.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make Flask application for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load pre-trained components\n",
    "tfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\n",
    "lsa = pickle.load(open('lsa.pkl', 'rb'))\n",
    "count_vectorizer = pickle.load(open('count_vectorizer.pkl', 'rb'))\n",
    "lda = pickle.load(open('lda.pkl', 'rb'))\n",
    "gbm_combined = pickle.load(open('gbm_combined.pkl', 'rb'))\n",
    "\n",
    "@app.route('/predict/combined', methods=['POST'])\n",
    "def predict_combined():\n",
    "    \"\"\" Handle POST request with JSON input and return prediction in structured format. \"\"\"\n",
    "    data = request.get_json()\n",
    "\n",
    "    if not data or ('title' not in data and 'body' not in data):\n",
    "        return jsonify({'status': 'error', 'message': 'No title or body provided'}), 400\n",
    "\n",
    "    title = data.get('title', '')  # Default to empty string if missing\n",
    "    body = data.get('body', '')\n",
    "\n",
    "    raw_text = f\"{title} {body}\".strip()\n",
    "\n",
    "    if not raw_text:\n",
    "        return jsonify({'status': 'error', 'message': 'Title and body are empty'}), 400\n",
    "\n",
    "    # Transform input text\n",
    "    X_tfidf_input = tfidf_vectorizer.transform([raw_text])\n",
    "    X_lsa_input = lsa.transform(X_tfidf_input)\n",
    "    X_lsa_input_sparse = csr_matrix(X_lsa_input)\n",
    "    X_tf_input = count_vectorizer.transform([raw_text])\n",
    "    X_lda_input = lda.transform(X_tf_input)\n",
    "    X_lda_input_sparse = csr_matrix(X_lda_input)\n",
    "\n",
    "    # Combine representations\n",
    "    X_combined_input = hstack([X_tfidf_input, X_lsa_input_sparse, X_lda_input_sparse]).tocsr()\n",
    "\n",
    "    # Predict probability\n",
    "    predicted_probability = gbm_combined.predict_proba(X_combined_input)[0]\n",
    "    bug_prob = predicted_probability[1]  # Assuming index 1 is for \"bug\"\n",
    "\n",
    "    return jsonify({\n",
    "        'status': 'success',\n",
    "        'predict_as': 'bug' if bug_prob > 0.5 else 'not bug',\n",
    "        'bug_prob': bug_prob\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run application here\n",
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hand on 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Page 50-51 of Handout #7\n",
    "Page 53 of Handout #7\n",
    "Page 60-67 of Handout #7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna : Page 50\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def objective(trial):\n",
    "    dtrain = lgb.Dataset(X_tfidf_fit_train, label=y_fit_train)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(param, dtrain)\n",
    "    preds = gbm.predict(X_tfidf_fit_test)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = metrics.roc_auc_score(y_fit_test, pred_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "trial = study.best_trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 51\n",
    "gbm_model = lgb.LGBMClassifier(**trial.params)\n",
    "\n",
    "precision_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring=\"precision_macro\"\n",
    ").mean()\n",
    "\n",
    "recall_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring=\"recall_macro\"\n",
    ").mean()\n",
    "\n",
    "f1_cv_score = model_selection.cross_val_score(\n",
    "    gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring=\"f1_macro\"\n",
    ").mean()\n",
    "\n",
    "print(f\"CV: p:{precision_cv_score:.4f} r:{recall_cv_score:.4f} f:{f1_cv_score:.4f}\")\n",
    "\n",
    "gbm_model.fit(\n",
    "    X_tfidf_fit_train, y_fit_train, \n",
    "    eval_set=[(X_tfidf_fit_test, y_fit_test)], \n",
    "    eval_metric=\"AUC\"\n",
    ")\n",
    "\n",
    "precision_test_score = metrics.precision_score(\n",
    "    gbm_model.predict(X_tfidf_blindtest), y_blindtest, average=\"macro\"\n",
    ")\n",
    "\n",
    "recall_test_score = metrics.recall_score(\n",
    "    gbm_model.predict(X_tfidf_blindtest), y_blindtest, average=\"macro\"\n",
    ")\n",
    "\n",
    "f1_test_score = metrics.f1_score(\n",
    "    gbm_model.predict(X_tfidf_blindtest), y_blindtest, average=\"macro\"\n",
    ")\n",
    "\n",
    "print(f\"test: p:{precision_test_score:.4f} r:{recall_test_score:.4f} f:{f1_test_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "\n",
    "with open(\"gbm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gbm_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 53: made into flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load pre-trained components into the app context\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    app.tfidf_vectorizer = pickle.load(f)\n",
    "with open('lsa.pkl', 'rb') as f:\n",
    "    app.lsa_model = pickle.load(f)\n",
    "with open('count_vectorizer.pkl', 'rb') as f:\n",
    "    app.count_vectorizer = pickle.load(f)\n",
    "with open('lda.pkl', 'rb') as f:\n",
    "    app.lda_model = pickle.load(f)\n",
    "with open('gbm_model.pkl', 'rb') as f:\n",
    "    app.optimized_model = pickle.load(f)\n",
    "    \n",
    "@app.route('/predict_optimized_tfidf_lda', methods=['POST'])\n",
    "def predict_optimized_tfidf_lda():\n",
    "    # Parse JSON from the request body\n",
    "    data = request.get_json()\n",
    "    if not data or 'title' not in data:\n",
    "        return jsonify({'status': 'error', 'message': 'Missing required parameter: title'}), 400\n",
    "\n",
    "    title = data['title']\n",
    "    \n",
    "    try:\n",
    "        # Transform the input text using vectorizers and models\n",
    "        tfidf_data = app.tfidf_vectorizer.transform([preprocess(title)])\n",
    "        lsa_data = app.lsa_model.transform(tfidf_data)\n",
    "        lda_data = app.lda_model.transform(app.count_vectorizer.transform([preprocess(title)]))\n",
    "        \n",
    "        # Combine features from TF-IDF, LSA, and LDA\n",
    "        query_data = hstack([tfidf_data, csr_matrix(lsa_data), csr_matrix(lda_data)]).tocsr()\n",
    "\n",
    "        # Check the number of features and chop off extra features if necessary\n",
    "        expected_features = app.optimized_model.n_features_in_  # Get the expected number of features from the model\n",
    "        if query_data.shape[1] > expected_features:\n",
    "            query_data = query_data[:, :expected_features]  # Slice the data to match the expected number of features\n",
    "\n",
    "        # Predict probabilities using the optimized model\n",
    "        preds = app.optimized_model.predict_proba(query_data)\n",
    "        bug_prob = preds[:, 1][0]  # Assuming the positive class is at index 1\n",
    "\n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'predict_as': 'bug' if bug_prob > 0.5 else 'not bug',\n",
    "            'bug_prob': bug_prob\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # If there is an error in the process, return the error message\n",
    "        return jsonify({'status': 'error', 'message': str(e)}), 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
