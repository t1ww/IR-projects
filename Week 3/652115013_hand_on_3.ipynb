{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib as plt\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from hand on 1\n",
    "def get_and_clean_data():\n",
    "    data = pd.read_csv('../Week 1/resource/software_developer_united_states_1971_20191023_1.csv')\n",
    "    description = data['job_description']\n",
    "    cleaned_description = description.apply(lambda s: s.translate(str.maketrans('', '', string.punctuation + u'\\xa0')))\n",
    "    cleaned_description = cleaned_description.apply(lambda s: s.lower())\n",
    "    cleaned_description = cleaned_description.apply(lambda s: s.translate(str.maketrans(string.whitespace, ' '*len(string.whitespace), '')))\n",
    "    cleaned_description = cleaned_description.drop_duplicates()\n",
    "    return cleaned_description\n",
    "\n",
    "def simple_tokenize(data):\n",
    "    cleaned_description = data.apply(lambda s: [x.strip() for x in s.split()])\n",
    "    return cleaned_description\n",
    "\n",
    "def parse_job_description():\n",
    "    cleaned_description = get_and_clean_data()\n",
    "    cleaned_description = simple_tokenize(cleaned_description)\n",
    "    return cleaned_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Page 74, 75 (Hand_out 2) : Activity â€” performance benchmark \n",
    "    - `Set operations vs list operation in stemming`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation setup\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "# (Given)\n",
    "def set_operation(sample_size):\n",
    "    cleaned_description = get_and_clean_data()[:sample_size]\n",
    "\n",
    "    #replace non alphabets with spaces, and collapse spaces\n",
    "    cleaned_description = cleaned_description.apply(lambda s: re.sub(r'[^A-Za-z]', ' ', s))\n",
    "    cleaned_description = cleaned_description.apply(lambda s: re.sub(r'\\s+', ' ', s))\n",
    "\n",
    "    #tokenize\n",
    "    tokenized_description = cleaned_description.apply(lambda s: word_tokenize(s))\n",
    "\n",
    "    #remove stop words\n",
    "    stop_dict = set(stopwords.words())\n",
    "    sw_removed_description = tokenized_description.apply(lambda s: list(OrderedSet(s) - stop_dict))\n",
    "    sw_removed_description = sw_removed_description.apply(lambda s: [word for word in s if len(word)>2])\n",
    "\n",
    "    #create stem caches\n",
    "    concated = np.unique(np.concatenate([s for s in tokenized_description.values]))\n",
    "    stem_cache = {}\n",
    "    ps = PorterStemmer()\n",
    "    for s in concated:\n",
    "        stem_cache[s] = ps.stem(s)\n",
    "\n",
    "    #stem\n",
    "    stemmed_description = sw_removed_description.apply(lambda s: [stem_cache[w] for w in s])\n",
    "\n",
    "    return stemmed_description\n",
    "\n",
    "def list_operation(sample_size):\n",
    "    cleaned_description = get_and_clean_data()[:sample_size]\n",
    "\n",
    "    #replace non alphabets with spaces, and collapse spaces\n",
    "    cleaned_description = cleaned_description.apply(lambda s: re.sub(r'[^A-Za-z]', ' ', s))\n",
    "    cleaned_description = cleaned_description.apply(lambda s: re.sub(r'\\s+', ' ', s))\n",
    "\n",
    "    #tokenize\n",
    "    tokenized_description = cleaned_description.apply(lambda s: word_tokenize(s))\n",
    "\n",
    "    #remove stop words\n",
    "    stop_dict = list(stopwords.words())\n",
    "    sw_removed_description = tokenized_description.apply(lambda s: [word for word in s if word not in stop_dict])\n",
    "    sw_removed_description = sw_removed_description.apply(lambda s: [word for word in s if len(word)>2])\n",
    "\n",
    "    #create stem caches\n",
    "    concated = np.unique(np.concatenate([s for s in tokenized_description.values]))\n",
    "    stem_cache = {}\n",
    "    ps = PorterStemmer()\n",
    "    for s in concated:\n",
    "        stem_cache[s] = ps.stem(s)\n",
    "\n",
    "    #stem\n",
    "    stemmed_description = sw_removed_description.apply(lambda s: [stem_cache[w] for w in s])\n",
    "\n",
    "    return stemmed_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "t_set = []\n",
    "t_list = []\n",
    "for i in range(20, 61, 20):\n",
    "    t_set.append(timeit.timeit(lambda: set_operation(i)), number=1)\n",
    "    t_list.append(timeit.timeit(lambda: list_operation(i)), number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(20, 61, 20), t_set, label='Set Operation', marker='o')\n",
    "plt.plot(range(20, 61, 20), t_list, label='List Operation', marker='o')\n",
    "\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Set vs List Operations - Performance Benchmark')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
