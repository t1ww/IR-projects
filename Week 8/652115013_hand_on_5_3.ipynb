{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handout #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import multiprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Empty\n",
    "from queue import Queue\n",
    "from pathlib import Path\n",
    "from bs4.element import Comment\n",
    "from nltk.corpus import stopwords \n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `66 - 67` - Power iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 39 iterations: [0.16911688 0.04196419 0.25324048 0.04196419 0.2572186  0.17669667\n",
      " 0.05979897]\n"
     ]
    }
   ],
   "source": [
    "# Page 66\n",
    "x0 = np.matrix([1/7] * 7)\n",
    "P = np.matrix([\n",
    "        [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7],\n",
    "        [25/56, 3/140, 25/56, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 3/140, 3/140, 61/70, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 3/140, 25/56],\n",
    "        [25/56, 3/140, 3/140, 3/140, 3/140, 25 /56, 3/140],\n",
    "        [3/140, 3/140, 61/70, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 25/56, 3/140],\n",
    "    ])\n",
    "\n",
    "prev_Px = x0\n",
    "Px = x0*P\n",
    "i=0\n",
    "while(any(abs(np.asarray(prev_Px).flatten()-np.asarray(Px).flatten()) > 1e-8)):\n",
    "    i+=1\n",
    "    prev_Px = Px\n",
    "    Px = Px * P\n",
    "\n",
    "print('Converged in {0} iterations: {1}'.format(i, np.asarray(Px).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `68 - 70` - The PageRank score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 68,69\n",
    "class Pr:\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def url_extractor(self):\n",
    "        url_maps = {}\n",
    "        all_urls = set([])\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                all_urls.add(j['url'])\n",
    "                for s in j['url_lists']:\n",
    "                    all_urls.add(s)\n",
    "                url_maps[j['url']] = list(set(j['url_lists']))\n",
    "                all_urls = list(all_urls)\n",
    "        return url_maps, all_urls\n",
    "    \n",
    "    def pr_calc(self):\n",
    "        url_maps, all_urls = self.url_extractor()\n",
    "        url_matrix = pd.DataFrame(columns=all_urls,index=all_urls)\n",
    "\n",
    "        for url in url_maps:\n",
    "            if len(url_maps[url]) > 0 and len(all_urls) > 0:\n",
    "                url_matrix.loc[url] = (1 - self.alpha) * (1 / len(all_urls))\n",
    "                url_matrix.loc[url, url_maps[url]] = url_matrix.loc[url, url_maps[url]] + (self.alpha *(1/len(url_maps[url])))\n",
    "\n",
    "        url_matrix.loc[url_matrix.isnull().all(axis=1), :] = (1 / len(all_urls))\n",
    "\n",
    "        x0 = np.matrix([1 / len(all_urls)] * len(all_urls))\n",
    "        P = np.asmatrix(url_matrix.values)\n",
    "\n",
    "        prev_Px = x0\n",
    "        Px = x0 * P\n",
    "        i = 0\n",
    "        while (any(abs(np.asarray(prev_Px).flatten() - np.asarray(Px).flatten()) > 1e-8)):\n",
    "            i += 1\n",
    "            prev_Px = Px\n",
    "            Px = Px * P\n",
    "\n",
    "        print('Converged in {0} iterations: {1}'.format(i, np.around(np.asarray(Px).flatten().astype(float), 5)))\n",
    "\n",
    "        self.pr_result = pd.DataFrame(Px, columns=url_matrix.index, index=['score']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m Pr(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpr_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mPr.pr_calc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpr_calc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     url_maps, all_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     url_matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mall_urls,index\u001b[38;5;241m=\u001b[39mall_urls)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m url_maps:\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mPr.url_extractor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     14\u001b[0m     j \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrawled_folder, file)))\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mall_urls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_lists\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     17\u001b[0m         all_urls\u001b[38;5;241m.\u001b[39madd(s)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "s = Pr(alpha=0.85)\n",
    "s.pr_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           score\n",
      "https://www.cmu.ac.th/en/                           4.916690e-09\n",
      "https://www.cmu.ac.th/en/#content                   4.011912e-09\n",
      "https://www.cmu.ac.th/                              3.183319e-09\n",
      "https://shop.cmu.ac.th                              3.051538e-09\n",
      "https://shop.cmu.ac.th/                             3.030638e-09\n",
      "...                                                          ...\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "\n",
      "[41599 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Page 70\n",
    "print(s.pr_result.sort_values(by='score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pr_calc result:`\n",
    "\n",
    "[Insert image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `71 - 75` - Integrating the PageRank score with Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 71\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class IndexerWithPR:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f)\n",
    "        \n",
    "        self.es_client = Elasticsearch(\"localhost:9200\", basic_auth=(\"elastic\", \"7L4*ufX=xVOj7qa9LDj=\"), ca_certs=\"~/http_ca.crt\")\n",
    "        self.pr = Pr(alpha=0.85)\n",
    "\n",
    "    def run_indexer(self):\n",
    "        self.pr.pr_calc()\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='simple')\n",
    "        self.es_client.options(ignore_status=[400]).indices.create(index='simple')\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                j['id'] = j['url']\n",
    "                j['pagerank'] = self.pr.pr_result.loc[j['id']].score\n",
    "                print(j)\n",
    "            self.es_client.index(index='simple', body=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 73\n",
    "from flask import Flask, request\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.es_client = Elasticsearch(\"https://localhost:9200\", basic_auth=(\"elastic\",\"_Z9BSk2zcMuFD=-1LlAX\"), ca_certs=\"~/http_ca.crt\")\n",
    "\n",
    "@app.route('/search_es_pr', methods=['GET'])\n",
    "def search_es_pr():\n",
    "    start = time.time()\n",
    "    response_object = {'status': 'success'}\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term=argList['query'][0]\n",
    "    results = app.es_client.search(index='simple', source_excludes=['url_lists'], size=100,\n",
    "    query={\"script_score\": {\"query\": { \"match\": { \"text\": query_term } }, \"script\": {\"source\":\"_score * doc['pagerank'].value\"}}})\n",
    "    end = time.time()\n",
    "    total_hit = results['hits']['total']['value']\n",
    "    results_df = pd.DataFrame([[hit[\"_source\"]['title'], hit[\"_source\"]['url'], hit[\"_source\"]['text'][:100],hit[\"_score\"]] for hit in results['hits']['hits']], columns=['title', 'url', 'text', 'score'])\n",
    "\n",
    "    response_object['total_hit'] = total_hit\n",
    "    response_object['results'] = results_df.to_dict('records')\n",
    "    response_object['elapse'] = end - start\n",
    "\n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [03/Feb/2025 16:36:02] code 400, message Bad request version ('À\\x13À')\n",
      "127.0.0.1 - - [03/Feb/2025 16:36:02] \"\\x16\\x03\\x01\\x00ó\\x01\\x00\\x00ï\\x03\\x03Å2'\\x8fÂàé\\x82\\x8ePKÚ±^~Ö¦åéJk1w\\x9fÐ­à\\x9b¥ì9÷ è\\x1cSsÅD¼\\x92\\x88¼ÔO\\x19\\x10ÞNS\\x981Ýhë\\x05Á\\x06\\x14k\\x17w\\x92KF\\x00$\\x13\\x01\\x13\\x02\\x13\\x03À/À+À0À,Ì©Ì¨À\\x09À\\x13À\" 400 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `77` - assignment\n",
    "\n",
    "Create a web page resembling the main Google homepage (1 point)\n",
    "- Upon executing a query, the user will be redirected to a new web page resembling https://splitsearch.netlify.app/.\n",
    "    - Left-hand side: Display ranking results using Elasticsearch's default BM25 scoring combined\n",
    "with PageRank. (1 point)\n",
    "    - Right-hand side: Display ranking results using the customized TF-IDF scoring combined with\n",
    "PageRank. (1 point)\n",
    "- Display the number of results and the time taken for the query execution. (1 point)\n",
    "- Add an <b> HTML tag around the query term(s). Display only two or three sentences surrounding the query term. Limit the details to two lines for clarity. (1 point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
