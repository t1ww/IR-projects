{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handout #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import multiprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Empty\n",
    "from queue import Queue\n",
    "from pathlib import Path\n",
    "from bs4.element import Comment\n",
    "from nltk.corpus import stopwords \n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `66 - 67` - Power iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 39 iterations: [0.16911688 0.04196419 0.25324048 0.04196419 0.2572186  0.17669667\n",
      " 0.05979897]\n"
     ]
    }
   ],
   "source": [
    "# Page 66\n",
    "x0 = np.matrix([1/7] * 7)\n",
    "P = np.matrix([\n",
    "        [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7],\n",
    "        [25/56, 3/140, 25/56, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 3/140, 3/140, 61/70, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 3/140, 25/56],\n",
    "        [25/56, 3/140, 3/140, 3/140, 3/140, 25 /56, 3/140],\n",
    "        [3/140, 3/140, 61/70, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 25/56, 3/140],\n",
    "    ])\n",
    "\n",
    "prev_Px = x0\n",
    "Px = x0*P\n",
    "i=0\n",
    "while(any(abs(np.asarray(prev_Px).flatten()-np.asarray(Px).flatten()) > 1e-8)):\n",
    "    i+=1\n",
    "    prev_Px = Px\n",
    "    Px = Px * P\n",
    "\n",
    "print('Converged in {0} iterations: {1}'.format(i, np.asarray(Px).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `68 - 70` - The PageRank score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 68,69\n",
    "class Pr:\n",
    "    def __init__(self, alpha):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def url_extractor(self):\n",
    "        url_maps = {}\n",
    "        all_urls = set()\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(self.crawled_folder, file)) as f:\n",
    "                    j = json.load(f)\n",
    "                    \n",
    "                all_urls.add(j['url'])\n",
    "                for s in j['url_lists']:\n",
    "                    all_urls.add(s)\n",
    "\n",
    "                url_maps[j['url']] = list(set(j['url_lists']))\n",
    "        \n",
    "        all_urls = list(all_urls) \n",
    "        return url_maps, all_urls\n",
    "\n",
    "    def pr_calc(self):\n",
    "        url_maps, all_urls = self.url_extractor()\n",
    "        num_urls = len(all_urls)\n",
    "        index_map = {url: i for i, url in enumerate(all_urls)}  # Map URLs to indices\n",
    "\n",
    "        # Use NumPy array instead of Pandas DataFrame\n",
    "        url_matrix = np.full((num_urls, num_urls), (1 - self.alpha) / num_urls)\n",
    "\n",
    "        for url, links in url_maps.items():\n",
    "            if links:\n",
    "                i = index_map[url]\n",
    "                url_matrix[i, [index_map[l] for l in links]] += self.alpha / len(links)\n",
    "\n",
    "        # Handle rows with no outgoing links\n",
    "        dangling_nodes = np.where(url_matrix.sum(axis=1) == 0)[0]\n",
    "        url_matrix[dangling_nodes, :] = 1 / num_urls\n",
    "\n",
    "        # Convert to NumPy matrix for faster multiplication\n",
    "        P = np.asmatrix(url_matrix)\n",
    "        x0 = np.asmatrix(np.full((1, num_urls), 1 / num_urls))\n",
    "\n",
    "        prev_Px = x0\n",
    "        Px = x0 @ P  # Matrix multiplication\n",
    "        i = 0\n",
    "        while np.linalg.norm(prev_Px - Px, ord=1) > 1e-8:  # Faster stopping condition\n",
    "            i += 1\n",
    "            prev_Px = Px\n",
    "            Px = Px @ P\n",
    "\n",
    "        print(f'Converged in {i} iterations')\n",
    "        scores = np.asarray(Px).flatten()\n",
    "        self.pr_result = pd.DataFrame(scores, index=all_urls, columns=['score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m Pr(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpr_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mPr.pr_calc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m index_map \u001b[38;5;241m=\u001b[39m {url: i \u001b[38;5;28;01mfor\u001b[39;00m i, url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_urls)}  \u001b[38;5;66;03m# Map URLs to indices\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Use NumPy array instead of Pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m url_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_urls\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_urls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, links \u001b[38;5;129;01min\u001b[39;00m url_maps\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m links:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SE-IR\\Lib\\site-packages\\numpy\\core\\numeric.py:330\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    329\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order)\n\u001b[1;32m--> 330\u001b[0m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = Pr(alpha=0.85)\n",
    "s.pr_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 70\n",
    "print(s.pr_result.sort_values(by='score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `71 - 75` - Integrating the PageRank score with Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 71\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class IndexerWithPR:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f)\n",
    "        \n",
    "        self.es_client = Elasticsearch(\"localhost:9200\", basic_auth=(\"elastic\", \"7L4*ufX=xVOj7qa9LDj=\"), ca_certs=\"~/http_ca.crt\")\n",
    "        self.pr = Pr(alpha=0.85)\n",
    "\n",
    "    def run_indexer(self):\n",
    "        self.pr.pr_calc()\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='simple')\n",
    "        self.es_client.options(ignore_status=[400]).indices.create(index='simple')\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                j['id'] = j['url']\n",
    "                j['pagerank'] = self.pr.pr_result.loc[j['id']].score\n",
    "                print(j)\n",
    "            self.es_client.index(index='simple', body=j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `77` - assignment\n",
    "\n",
    "Create a web page resembling the main Google homepage (1 point)\n",
    "- Upon executing a query, the user will be redirected to a new web page resembling https://splitsearch.netlify.app/.\n",
    "    - Left-hand side: Display ranking results using Elasticsearch's default BM25 scoring combined\n",
    "with PageRank. (1 point)\n",
    "    - Right-hand side: Display ranking results using the customized TF-IDF scoring combined with\n",
    "PageRank. (1 point)\n",
    "- Display the number of results and the time taken for the query execution. (1 point)\n",
    "- Add an <b> HTML tag around the query term(s). Display only two or three sentences surrounding the query term. Limit the details to two lines for clarity. (1 point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
