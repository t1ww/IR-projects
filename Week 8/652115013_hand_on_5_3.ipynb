{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handout #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `66 - 67` - Power iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 39 iterations: [0.16911688 0.04196419 0.25324048 0.04196419 0.2572186  0.17669667\n",
      " 0.05979897]\n"
     ]
    }
   ],
   "source": [
    "# Page 66\n",
    "x0 = np.matrix([1/7] * 7)\n",
    "P = np.matrix([\n",
    "        [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7],\n",
    "        [25/56, 3/140, 25/56, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 3/140, 3/140, 61/70, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 3/140, 25/56],\n",
    "        [25/56, 3/140, 3/140, 3/140, 3/140, 25 /56, 3/140],\n",
    "        [3/140, 3/140, 61/70, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 25/56, 3/140],\n",
    "    ])\n",
    "\n",
    "prev_Px = x0\n",
    "Px = x0*P\n",
    "i=0\n",
    "while(any(abs(np.asarray(prev_Px).flatten()-np.asarray(Px).flatten()) > 1e-8)):\n",
    "    i+=1\n",
    "    prev_Px = Px\n",
    "    Px = Px * P\n",
    "\n",
    "print('Converged in {0} iterations: {1}'.format(i, np.asarray(Px).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `68 - 70` - The PageRank score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 68,69\n",
    "# PageRank class definition\n",
    "class Pr:\n",
    "    def __init__(self, alpha, max_depth=2):  # Add max_depth argument\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        self.alpha = alpha\n",
    "        self.max_depth = max_depth  # Store max depth\n",
    "\n",
    "    def url_extractor(self, current_depth=0):\n",
    "        # If the maximum depth has been reached, stop crawling\n",
    "        if current_depth >= self.max_depth:\n",
    "            return {}, set()\n",
    "\n",
    "        url_maps = {}\n",
    "        all_urls = set([])\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                all_urls.add(j['url'])\n",
    "                for s in j['url_lists']:\n",
    "                    all_urls.add(s)\n",
    "                url_maps[j['url']] = list(set(j['url_lists']))\n",
    "                \n",
    "        # Proceed to the next level (increase depth)\n",
    "        if current_depth < self.max_depth:\n",
    "            nested_url_maps = {}\n",
    "            nested_all_urls = set([])\n",
    "            for url in all_urls:\n",
    "                nested_url_maps, nested_all_urls = self.url_extractor(current_depth + 1)\n",
    "                url_maps.update(nested_url_maps)\n",
    "                all_urls.update(nested_all_urls)\n",
    "        \n",
    "        all_urls = list(all_urls)\n",
    "        return url_maps, all_urls\n",
    "\n",
    "    def pr_calc(self):\n",
    "        url_maps, all_urls = self.url_extractor()  # No need to pass depth here, it’s managed by default\n",
    "        url_matrix = pd.DataFrame(columns=all_urls, index=all_urls)\n",
    "        \n",
    "        for url in url_maps:\n",
    "            if len(url_maps[url]) > 0 and len(all_urls) > 0:\n",
    "                url_matrix.loc[url] = (1 - self.alpha) * (1 / len(all_urls))\n",
    "                url_matrix.loc[url, url_maps[url]] = url_matrix.loc[url, url_maps[url]] + (self.alpha * (1 / len(url_maps[url])))\n",
    "\n",
    "        url_matrix.loc[url_matrix.isnull().all(axis=1), :] = (1 / len(all_urls))\n",
    "\n",
    "        x0 = np.matrix([1 / len(all_urls)] * len(all_urls))\n",
    "        P = np.asmatrix(url_matrix.values)\n",
    "\n",
    "        prev_Px = x0\n",
    "        Px = x0 * P\n",
    "        i = 0\n",
    "        while (any(abs(np.asarray(prev_Px).flatten() - np.asarray(Px).flatten()) > 1e-8)):\n",
    "            i += 1\n",
    "            prev_Px = Px\n",
    "            Px = Px * P\n",
    "\n",
    "        print('Converged in {0} iterations: {1}'.format(i, np.around(np.asarray(Px).flatten().astype(float), 5)))\n",
    "\n",
    "        self.pr_result = pd.DataFrame(Px, columns=url_matrix.index, index=['score']).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Pr(alpha=0.85, max_depth=2)  # Setting max_depth to 2\n",
    "s.pr_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           score\n",
      "https://www.cmu.ac.th/en/                           4.916690e-09\n",
      "https://www.cmu.ac.th/en/#content                   4.011912e-09\n",
      "https://www.cmu.ac.th/                              3.183319e-09\n",
      "https://shop.cmu.ac.th                              3.051538e-09\n",
      "https://shop.cmu.ac.th/                             3.030638e-09\n",
      "...                                                          ...\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "\n",
      "[41599 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Page 70\n",
    "print(s.pr_result.sort_values(by='score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pr_calc result:`\n",
    "\n",
    "[Insert image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `71 - 75` - Integrating the PageRank score with Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 71\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class IndexerWithPR:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f)\n",
    "        \n",
    "        self.es_client = Elasticsearch(\"localhost:9200\", basic_auth=(\"elastic\", \"7L4*ufX=xVOj7qa9LDj=\"), ca_certs=\"~/http_ca.crt\")\n",
    "        self.pr = Pr(alpha=0.85)\n",
    "\n",
    "    def run_indexer(self):\n",
    "        self.pr.pr_calc()\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='simple')\n",
    "        self.es_client.options(ignore_status=[400]).indices.create(index='simple')\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                j['id'] = j['url']\n",
    "                j['pagerank'] = self.pr.pr_result.loc[j['id']].score\n",
    "                print(j)\n",
    "            self.es_client.index(index='simple', body=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 73\n",
    "from flask import Flask, request\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.es_client = Elasticsearch(\"https://localhost:9200\", basic_auth=(\"elastic\",\"_Z9BSk2zcMuFD=-1LlAX\"), ca_certs=\"~/http_ca.crt\")\n",
    "\n",
    "@app.route('/search_es_pr', methods=['GET'])\n",
    "def search_es_pr():\n",
    "    start = time.time()\n",
    "    response_object = {'status': 'success'}\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term=argList['query'][0]\n",
    "    results = app.es_client.search(index='simple', source_excludes=['url_lists'], size=100,\n",
    "    query={\"script_score\": {\"query\": { \"match\": { \"text\": query_term } }, \"script\": {\"source\":\"_score * doc['pagerank'].value\"}}})\n",
    "    end = time.time()\n",
    "    total_hit = results['hits']['total']['value']\n",
    "    results_df = pd.DataFrame([[hit[\"_source\"]['title'], hit[\"_source\"]['url'], hit[\"_source\"]['text'][:100],hit[\"_score\"]] for hit in results['hits']['hits']], columns=['title', 'url', 'text', 'score'])\n",
    "\n",
    "    response_object['total_hit'] = total_hit\n",
    "    response_object['results'] = results_df.to_dict('records')\n",
    "    response_object['elapse'] = end - start\n",
    "\n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [03/Feb/2025 16:36:02] code 400, message Bad request version ('À\\x13À')\n",
      "127.0.0.1 - - [03/Feb/2025 16:36:02] \"\\x16\\x03\\x01\\x00ó\\x01\\x00\\x00ï\\x03\\x03Å2'\\x8fÂàé\\x82\\x8ePKÚ±^~Ö¦åéJk1w\\x9fÐ­à\\x9b¥ì9÷ è\\x1cSsÅD¼\\x92\\x88¼ÔO\\x19\\x10ÞNS\\x981Ýhë\\x05Á\\x06\\x14k\\x17w\\x92KF\\x00$\\x13\\x01\\x13\\x02\\x13\\x03À/À+À0À,Ì©Ì¨À\\x09À\\x13À\" 400 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `77` - assignment\n",
    "\n",
    "Create a web page resembling the main Google homepage (1 point)\n",
    "- Upon executing a query, the user will be redirected to a new web page resembling https://splitsearch.netlify.app/.\n",
    "    - Left-hand side: Display ranking results using Elasticsearch's default BM25 scoring combined\n",
    "with PageRank. (1 point)\n",
    "    - Right-hand side: Display ranking results using the customized TF-IDF scoring combined with\n",
    "PageRank. (1 point)\n",
    "- Display the number of results and the time taken for the query execution. (1 point)\n",
    "- Add an <b> HTML tag around the query term(s). Display only two or three sentences surrounding the query term. Limit the details to two lines for clarity. (1 point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
