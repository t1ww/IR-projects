{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handout #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import multiprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Empty\n",
    "from queue import Queue\n",
    "from pathlib import Path\n",
    "from bs4.element import Comment\n",
    "from nltk.corpus import stopwords \n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `66 - 67` - Power iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 39 iterations: [0.16911688 0.04196419 0.25324048 0.04196419 0.2572186  0.17669667\n",
      " 0.05979897]\n"
     ]
    }
   ],
   "source": [
    "# Page 66\n",
    "x0 = np.matrix([1/7] * 7)\n",
    "P = np.matrix([\n",
    "        [1/7, 1/7, 1/7, 1/7, 1/7, 1/7, 1/7],\n",
    "        [25/56, 3/140, 25/56, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 3/140, 3/140, 61/70, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 3/140, 25/56],\n",
    "        [25/56, 3/140, 3/140, 3/140, 3/140, 25 /56, 3/140],\n",
    "        [3/140, 3/140, 61/70, 3/140, 3/140, 3/140, 3/140],\n",
    "        [3/140, 3/140, 25/56, 3/140, 3/140, 25/56, 3/140],\n",
    "    ])\n",
    "\n",
    "prev_Px = x0\n",
    "Px = x0*P\n",
    "i=0\n",
    "while(any(abs(np.asarray(prev_Px).flatten()-np.asarray(Px).flatten()) > 1e-8)):\n",
    "    i+=1\n",
    "    prev_Px = Px\n",
    "    Px = Px * P\n",
    "\n",
    "print('Converged in {0} iterations: {1}'.format(i, np.asarray(Px).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `68 - 70` - The PageRank score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 68,69\n",
    "class Pr:\n",
    "    def __init__(self, alpha):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def url_extractor(self):\n",
    "        url_maps = {}\n",
    "        all_urls = set()\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(self.crawled_folder, file)) as f:\n",
    "                    j = json.load(f)\n",
    "                    \n",
    "                all_urls.add(j['url'])\n",
    "                for s in j['url_lists']:\n",
    "                    all_urls.add(s)\n",
    "\n",
    "                url_maps[j['url']] = list(set(j['url_lists']))\n",
    "        \n",
    "        all_urls = list(all_urls) \n",
    "        return url_maps, all_urls\n",
    "\n",
    "    def pr_calc(self):\n",
    "        url_maps, all_urls = self.url_extractor()\n",
    "        num_urls = len(all_urls)\n",
    "        index_map = {url: i for i, url in enumerate(all_urls)}  # Map URLs to indices\n",
    "\n",
    "        # Use NumPy array instead of Pandas DataFrame\n",
    "        url_matrix = np.full((num_urls, num_urls), (1 - self.alpha) / num_urls)\n",
    "\n",
    "        for url, links in url_maps.items():\n",
    "            if links:\n",
    "                i = index_map[url]\n",
    "                url_matrix[i, [index_map[l] for l in links]] += self.alpha / len(links)\n",
    "\n",
    "        # Handle rows with no outgoing links\n",
    "        dangling_nodes = np.where(url_matrix.sum(axis=1) == 0)[0]\n",
    "        url_matrix[dangling_nodes, :] = 1 / num_urls\n",
    "\n",
    "        # Convert to NumPy matrix for faster multiplication\n",
    "        P = np.asmatrix(url_matrix)\n",
    "        x0 = np.asmatrix(np.full((1, num_urls), 1 / num_urls))\n",
    "\n",
    "        prev_Px = x0\n",
    "        Px = x0 @ P  # Matrix multiplication\n",
    "        i = 0\n",
    "        while np.linalg.norm(prev_Px - Px, ord=1) > 1e-8:  # Faster stopping condition\n",
    "            i += 1\n",
    "            prev_Px = Px\n",
    "            Px = Px @ P\n",
    "\n",
    "        print(f'Converged in {i} iterations')\n",
    "        scores = np.asarray(Px).flatten()\n",
    "        self.pr_result = pd.DataFrame(scores, index=all_urls, columns=['score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m Pr(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpr_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 41\u001b[0m, in \u001b[0;36mPr.pr_calc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m Px \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m*\u001b[39m P\n\u001b[0;32m     40\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mabs\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_Px\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(Px)\u001b[38;5;241m.\u001b[39mflatten()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-8\u001b[39m):\n\u001b[0;32m     42\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     43\u001b[0m     prev_Px \u001b[38;5;241m=\u001b[39m Px\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = Pr(alpha=0.85)\n",
    "s.pr_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                           score\n",
      "https://www.cmu.ac.th/en/                           4.916690e-09\n",
      "https://www.cmu.ac.th/en/#content                   4.011912e-09\n",
      "https://www.cmu.ac.th/                              3.183319e-09\n",
      "https://shop.cmu.ac.th                              3.051538e-09\n",
      "https://shop.cmu.ac.th/                             3.030638e-09\n",
      "...                                                          ...\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "https://www.cmu.ac.th/content/Article/2024/16fa...  2.217631e-12\n",
      "\n",
      "[41599 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Page 70\n",
    "print(s.pr_result.sort_values(by='score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pr_calc result:`\n",
    "\n",
    "[Insert image]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `71 - 75` - Integrating the PageRank score with Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 71\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class IndexerWithPR:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.crawled_folder = Path(os.path.abspath('')).parent / 'crawled/'\n",
    "        with open(self.crawled_folder / 'url_list.pickle', 'rb') as f:\n",
    "            self.file_mapper = pickle.load(f)\n",
    "        \n",
    "        self.es_client = Elasticsearch(\"localhost:9200\", basic_auth=(\"elastic\", \"7L4*ufX=xVOj7qa9LDj=\"), ca_certs=\"~/http_ca.crt\")\n",
    "        self.pr = Pr(alpha=0.85)\n",
    "\n",
    "    def run_indexer(self):\n",
    "        self.pr.pr_calc()\n",
    "        self.es_client.options(ignore_status=[400, 404]).indices.delete(index='simple')\n",
    "        self.es_client.options(ignore_status=[400]).indices.create(index='simple')\n",
    "\n",
    "        for file in os.listdir(self.crawled_folder):\n",
    "            if file.endswith(\".txt\"):\n",
    "                j = json.load(open(os.path.join(self.crawled_folder, file)))\n",
    "                j['id'] = j['url']\n",
    "                j['pagerank'] = self.pr.pr_result.loc[j['id']].score\n",
    "                print(j)\n",
    "            self.es_client.index(index='simple', body=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 73\n",
    "from flask import Flask, request\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.es_client = Elasticsearch(\"https://localhost:9200\", basic_auth=(\"elastic\",\"_Z9BSk2zcMuFD=-1LlAX\"), ca_certs=\"~/http_ca.crt\")\n",
    "\n",
    "@app.route('/search_es_pr', methods=['GET'])\n",
    "def search_es_pr():\n",
    "    start = time.time()\n",
    "    response_object = {'status': 'success'}\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    query_term=argList['query'][0]\n",
    "    results = app.es_client.search(index='simple', source_excludes=['url_lists'], size=100,\n",
    "    query={\"script_score\": {\"query\": { \"match\": { \"text\": query_term } }, \"script\": {\"source\":\"_score * doc['pagerank'].value\"}}})\n",
    "    end = time.time()\n",
    "    total_hit = results['hits']['total']['value']\n",
    "    results_df = pd.DataFrame([[hit[\"_source\"]['title'], hit[\"_source\"]['url'], hit[\"_source\"]['text'][:100],hit[\"_score\"]] for hit in results['hits']['hits']], columns=['title', 'url', 'text', 'score'])\n",
    "\n",
    "    response_object['total_hit'] = total_hit\n",
    "    response_object['results'] = results_df.to_dict('records')\n",
    "    response_object['elapse'] = end - start\n",
    "\n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [03/Feb/2025 16:36:02] code 400, message Bad request version ('À\\x13À')\n",
      "127.0.0.1 - - [03/Feb/2025 16:36:02] \"\\x16\\x03\\x01\\x00ó\\x01\\x00\\x00ï\\x03\\x03Å2'\\x8fÂàé\\x82\\x8ePKÚ±^~Ö¦åéJk1w\\x9fÐ­à\\x9b¥ì9÷ è\\x1cSsÅD¼\\x92\\x88¼ÔO\\x19\\x10ÞNS\\x981Ýhë\\x05Á\\x06\\x14k\\x17w\\x92KF\\x00$\\x13\\x01\\x13\\x02\\x13\\x03À/À+À0À,Ì©Ì¨À\\x09À\\x13À\" 400 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `77` - assignment\n",
    "\n",
    "Create a web page resembling the main Google homepage (1 point)\n",
    "- Upon executing a query, the user will be redirected to a new web page resembling https://splitsearch.netlify.app/.\n",
    "    - Left-hand side: Display ranking results using Elasticsearch's default BM25 scoring combined\n",
    "with PageRank. (1 point)\n",
    "    - Right-hand side: Display ranking results using the customized TF-IDF scoring combined with\n",
    "PageRank. (1 point)\n",
    "- Display the number of results and the time taken for the query execution. (1 point)\n",
    "- Add an <b> HTML tag around the query term(s). Display only two or three sentences surrounding the query term. Limit the details to two lines for clarity. (1 point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
