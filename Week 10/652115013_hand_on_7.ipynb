{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand on 7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `22-27` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 22 : (but optimized?)\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Precompile regex patterns (compiled only once)\n",
    "_var_addr_pattern = re.compile(r'.*0x[0-9a-f].*')\n",
    "_name_with_number_pattern = re.compile(r'.*[a-f]*:[0-9]*')\n",
    "_number_start_one_char_pattern = re.compile(r'[a-f][0-9].*')\n",
    "_number_start_three_char_pattern = re.compile(r'[a-f]{3}[0-9].*')\n",
    "_number_sub_pattern = re.compile(r'[\\\\/;:_-]')\n",
    "\n",
    "def preprocess(text, stopword_set, stemmer):\n",
    "    # Remove punctuation and unwanted characters, then lowercase the text\n",
    "    translation_table = str.maketrans('', '', '!\"#$%&\\'()*+,.<=>?@[]^`{|}~' + u'\\xa0')\n",
    "    cleaned_text = text.translate(translation_table).lower()\n",
    "    \n",
    "    # Replace all whitespace characters with a single space\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans(string.whitespace, ' ' * len(string.whitespace), ''))\n",
    "    \n",
    "    # Split the text only once\n",
    "    tokens = cleaned_text.split()\n",
    "    new_tokens = []\n",
    "    \n",
    "    # Process each token with all transformation rules in one pass\n",
    "    for token in tokens:\n",
    "        if '_' in token:\n",
    "            new_tokens.append('_variable_with_underscore')\n",
    "        elif '-' in token:\n",
    "            new_tokens.append('_variable_with_dash')\n",
    "        elif len(token) > 15 and token[0] != '#':\n",
    "            new_tokens.append('_long_variable_name')\n",
    "        elif token.startswith('http') and '/' in token:\n",
    "            new_tokens.append('_weburl')\n",
    "        elif _number_sub_pattern.sub('', token).isdigit():\n",
    "            new_tokens.append('_number')\n",
    "        elif _var_addr_pattern.match(token):\n",
    "            new_tokens.append('_variable_with_address')\n",
    "        elif _name_with_number_pattern.match(token):\n",
    "            new_tokens.append('_name_with_number')\n",
    "        elif _number_start_one_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_one_character')\n",
    "        elif _number_start_three_char_pattern.match(token):\n",
    "            new_tokens.append('_number_starts_with_three_characters')\n",
    "        elif any(c.isdigit() for c in token) and token.startswith('v'):\n",
    "            new_tokens.append('_version')\n",
    "        elif ('\\\\' in token or '/' in token) and ':' not in token:\n",
    "            new_tokens.append('_localpath')\n",
    "        elif token.endswith('px'):\n",
    "            new_tokens.append('_image_size')\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    # Remove stopwords and tokens shorter than 3 characters, then perform stemming\n",
    "    final_tokens = [stemmer.stem(tok) for tok in new_tokens if tok not in stopword_set and len(tok) > 2]\n",
    "    return ' '.join(final_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to initialize global variables in worker processes\n",
    "def initialize_pool(stopword_set_arg, stemmer_arg):\n",
    "    global stopword_set, stemmer\n",
    "    stopword_set = stopword_set_arg\n",
    "    stemmer = stemmer_arg\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_json('resource/embold_train.json')\n",
    "\n",
    "# Label transformations\n",
    "dataset.loc[dataset['label'] > 0, 'label'] = -1\n",
    "dataset.loc[dataset['label'] == 0, 'label'] = 1\n",
    "dataset.loc[dataset['label'] == -1, 'label'] = 0\n",
    "\n",
    "# Define stopwords and stemmer\n",
    "stopwords_set = set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Initialize the pool of workers with the optimized preprocess globals\n",
    "pool = Pool(8, initializer=initialize_pool, initargs=(stopwords_set, ps))\n",
    "\n",
    "# Preprocess the dataset using multiprocessing\n",
    "cleaned_title = pool.map(preprocess, dataset['title'])\n",
    "cleaned_body = pool.map(preprocess, dataset['body'])\n",
    "\n",
    "# Combine the cleaned texts into a DataFrame\n",
    "data_texts = pd.DataFrame({'title': cleaned_title, 'body': cleaned_body})\n",
    "\n",
    "# Labels\n",
    "y = dataset['label']\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle is now given, Skipping first 2 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipped page 23\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "# Read pickle\n",
    "with open('resource/data_texts.pickle', 'rb') as f:\n",
    "    data_texts = pickle.load(f)\n",
    "with open('resource/embold_train_y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: p:0.7956 r:0.7932 f:0.7942\n"
     ]
    }
   ],
   "source": [
    "# Page 25 : Walkthroughs â€“ cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Split the dataset into training and blindtest (testing) sets\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = train_test_split(data_texts, y, test_size=0.1)\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with unigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "# Concatenate the title and body columns\n",
    "# Assuming 'data_texts' contains both 'title' and 'body' columns\n",
    "data_texts_combined = data_texts['title'] + ' ' + data_texts['body']\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the concatenated text (title + body)\n",
    "tfidf_vectorizer.fit(data_texts_combined)\n",
    "\n",
    "# Transform the training and blindtest data\n",
    "X_tfidf_fit = tfidf_vectorizer.transform(data_fit['title'] + ' ' + data_fit['body'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'] + ' ' + data_blindtest['body'])\n",
    "\n",
    "# Initialize the model\n",
    "gbm_model = lgb.LGBMClassifier()\n",
    "\n",
    "# Cross-validation for precision, recall, and f1 score\n",
    "precision_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='precision_macro').mean()\n",
    "recall_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='recall_macro').mean()\n",
    "f1_cv_score = model_selection.cross_val_score(gbm_model, X_tfidf_fit, y_fit, cv=5, n_jobs=-2, scoring='f1_macro').mean()\n",
    "\n",
    "# Output the results\n",
    "print('CV: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_cv_score, recall_cv_score, f1_cv_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 32696, number of negative: 40804\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.093978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 85032\n",
      "[LightGBM] [Info] Number of data points in the train set: 73500, number of used features: 1863\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444844 -> initscore=-0.221527\n",
      "[LightGBM] [Info] Start training from score -0.221527\n",
      "test: p:0.7432 r:0.7648 f:0.7461\n"
     ]
    }
   ],
   "source": [
    "# Page 26 : Modelling\n",
    "from sklearn import metrics\n",
    "\n",
    "data_fit, data_blindtest, y_fit, y_blindtest = model_selection.train_test_split(data_texts, y, test_size=0.3)\n",
    "\n",
    "data_fit_train, data_fit_test, y_fit_train, y_fit_test = model_selection.train_test_split(data_fit, y_fit, test_size=0.3)\n",
    "X_tfidf_fit_train = tfidf_vectorizer.transform(data_fit_train['title'])\n",
    "X_tfidf_fit_test = tfidf_vectorizer.transform(data_fit_test['title'])\n",
    "X_tfidf_blindtest = tfidf_vectorizer.transform(data_blindtest['title'])\n",
    "\n",
    "gbm_model.fit(X_tfidf_fit_train, y_fit_train, eval_set=[(X_tfidf_fit_test, y_fit_test)], eval_metric='AUC')\n",
    "\n",
    "precision_test_score = metrics.precision_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "recall_test_score = metrics.recall_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "f1_test_score = metrics.f1_score(gbm_model.predict(X_tfidf_blindtest), y_blindtest, average='macro')\n",
    "\n",
    "print('test: p:{0:.4f} r:{1:.4f} f:{2:.4f}'.format(precision_test_score, recall_test_score, f1_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Page 27 : Save the models\n",
    "pickle.dump(tfidf_vectorizer, open('resource/github_bug_prediction_tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(gbm_model, open('resource/github_bug_prediction_basic_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `28-29` of Handout #7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  # Using PorterStemmer\n",
    "\n",
    "# Initialize the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load resources\n",
    "app.tfidf_vectorizer = pickle.load(open('resource/github_bug_prediction_tfidf_vectorizer.pkl', 'rb'))\n",
    "app.basic_model = pickle.load(open('resource/github_bug_prediction_basic_model.pkl', 'rb'))\n",
    "app.stopword_set = set(stopwords.words('english'))\n",
    "app.stemmer = PorterStemmer()  # Correctly initialize PorterStemmer\n",
    "\n",
    "@app.route('/predict_basic', methods=['GET'])\n",
    "def predict_basic_get():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get query parameters\n",
    "    argList = request.args.to_dict(flat=False)\n",
    "    title = argList.get('title', [None])[0]  # Safely get title\n",
    "    body = argList.get('body', [None])[0]  # Safely get body\n",
    "\n",
    "    if not title or not body:  # Validate input\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Apply preprocessing to the title\n",
    "    processed_title = preprocess(title)  # Using preprocess function on the title\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n",
    "\n",
    "@app.route('/predict_basic', methods=['POST'])\n",
    "def predict_basic_post():\n",
    "    response_object = {'status': 'success'}\n",
    "    \n",
    "    # Get data from JSON body\n",
    "    data = request.get_json()  # Parse JSON body\n",
    "    title = data.get('title')  \n",
    "    body = data.get('body')    \n",
    "\n",
    "    if not title or not body:  # Validate if title or body are missing\n",
    "        response_object['status'] = 'error'\n",
    "        response_object['message'] = 'Missing title or body'\n",
    "        return response_object\n",
    "\n",
    "    # Initialize stopword_set and stemmer\n",
    "    stopword_set = set(stopwords.words('english'))  # Assuming you are using NLTK stopwords\n",
    "    stemmer = PorterStemmer()  # Using PorterStemmer\n",
    "    \n",
    "    # Process title using preprocess with stopword_set and stemmer\n",
    "    processed_title = preprocess(title, stopword_set, stemmer)\n",
    "\n",
    "    # Predict bug likelihood\n",
    "    predict = app.basic_model.predict_proba(hstack([app.tfidf_vectorizer.transform([processed_title])]))\n",
    "\n",
    "    response_object['predict_as'] = 'bug' if predict[0][1] > 0.5 else 'not bug'\n",
    "    response_object['bug_prob'] = predict[0][1]\n",
    "    \n",
    "    return response_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [17/Feb/2025 17:38:15] \"POST /predict_basic HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Page `39` of Handout #7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE-IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
